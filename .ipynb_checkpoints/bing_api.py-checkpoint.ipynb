{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/takayuki\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 20] Not a directory: '/Users/takayuki/Document/machine_learning/bing_api.py.ipynb'\n",
      "/Users/takayuki\n"
     ]
    }
   ],
   "source": [
    "cd /Users/takayuki/Document/machine_learning/bing_api.py.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 20] Not a directory: '/Users/takayuki/Document/machine_learning/bing_api.py.ipynb'\n",
      "/Users/takayuki\n"
     ]
    }
   ],
   "source": [
    "cd \"/Users/takayuki/Document/machine_learning/bing_api.py.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/takayuki'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/takayuki/Document/machine_learning\n"
     ]
    }
   ],
   "source": [
    "cd /Users/takayuki/Document/machine_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5f7860917251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"&r=(http.+)&p=\"\u001b[0m \u001b[0;31m# extract an URL of image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0munquoted_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munquote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'contentUrl'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mimg_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munquoted_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'value'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import http.client\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import urllib\n",
    "import hashlib\n",
    "import sha3\n",
    "\n",
    "\n",
    "def make_dir(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "\n",
    "def make_correspondence_table(correspondence_table, original_url, hashed_url):\n",
    "    \"\"\"Create reference table of hash value and original URL.\n",
    "    \"\"\"\n",
    "    correspondence_table[original_url] = hashed_url\n",
    "\n",
    "\n",
    "def make_img_path(save_dir_path, url):\n",
    "    \"\"\"Hash the image url and create the path\n",
    "\n",
    "    Args:\n",
    "        save_dir_path (str): Path to save image dir.\n",
    "        url (str): An url of image.\n",
    "\n",
    "    Returns:\n",
    "        Path of hashed image URL.\n",
    "    \"\"\"\n",
    "    save_img_path = os.path.join(save_dir_path, 'imgs')\n",
    "    make_dir(save_img_path)\n",
    "\n",
    "    file_extension = os.path.splitext(url)[-1]\n",
    "    if file_extension.lower() in ('.jpg', '.jpeg', '.gif', '.png', '.bmp'):\n",
    "        encoded_url = url.encode('utf-8') # required encoding for hashed\n",
    "        hashed_url = hashlib.sha3_256(encoded_url).hexdigest()\n",
    "        full_path = os.path.join(save_img_path, hashed_url + file_extension.lower())\n",
    "\n",
    "        make_correspondence_table(correspondence_table, url, hashed_url)\n",
    "\n",
    "        return full_path\n",
    "    else:\n",
    "        raise ValueError('Not applicable file extension')\n",
    "\n",
    "\n",
    "def download_image(url, timeout=10):\n",
    "    response = requests.get(url, allow_redirects=True, timeout=timeout)\n",
    "    if response.status_code != 200:\n",
    "        error = Exception(\"HTTP status: \" + response.status_code)\n",
    "        raise error\n",
    "\n",
    "    content_type = response.headers[\"content-type\"]\n",
    "    if 'image' not in content_type:\n",
    "        error = Exception(\"Content-Type: \" + content_type)\n",
    "        raise error\n",
    "\n",
    "    return response.content\n",
    "\n",
    "\n",
    "def save_image(filename, image):\n",
    "    with open(filename, \"wb\") as fout:\n",
    "        fout.write(image)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_dir_path = '/Users/takayuki/Document/machine_learning'\n",
    "    make_dir(save_dir_path)\n",
    "\n",
    "    num_imgs_required = 1000 # Number of images you want. The number to be divisible by 'num_imgs_per_transaction'\n",
    "    num_imgs_per_transaction = 150 # default 30, Max 150\n",
    "    offset_count = math.floor(num_imgs_required / num_imgs_per_transaction)\n",
    "\n",
    "    url_list = []\n",
    "    correspondence_table = {}\n",
    "\n",
    "    headers = {\n",
    "        # Request headers\n",
    "        'Content-Type': 'multipart/form-data',\n",
    "        'Ocp-Apim-Subscription-Key': '966ac340fc7344af8071dd29862e9a01', # API key\n",
    "    }\n",
    "\n",
    "    for offset in range(offset_count):\n",
    "\n",
    "        params = urllib.parse.urlencode({\n",
    "            # Request parameters\n",
    "            'q': 'çŒ«',\n",
    "            'mkt': 'ja-JP',\n",
    "            'count': num_imgs_per_transaction,\n",
    "            'offset': offset * num_imgs_per_transaction # increment offset by 'num_imgs_per_transaction' (for example 0, 150, 300)\n",
    "        })\n",
    "\n",
    "        try:\n",
    "            conn = http.client.HTTPSConnection('api.cognitive.microsoft.com')\n",
    "            conn.request(\"POST\", \"/bing/v5.0/images/search?%s\" % params, \"{body}\", headers)\n",
    "            response = conn.getresponse()\n",
    "            data = response.read()\n",
    "\n",
    "            save_res_path = os.path.join(save_dir_path, 'pickle_files')\n",
    "            make_dir(save_res_path)\n",
    "            with open(os.path.join(save_res_path, '{}.pickle'.format(offset)), mode='wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "\n",
    "            conn.close()\n",
    "        except Exception as err:\n",
    "            print(\"[Errno {0}] {1}\".format(err.errno, err.strerror))\n",
    "\n",
    "        else:\n",
    "            decode_res = data.decode('utf-8')\n",
    "            data = json.loads(decode_res)\n",
    "\n",
    "            pattern = r\"&r=(http.+)&p=\" # extract an URL of image\n",
    "\n",
    "            for values in data['value']:\n",
    "                unquoted_url = urllib.parse.unquote(values['contentUrl'])\n",
    "                img_url = re.search(pattern, unquoted_url)\n",
    "                if img_url:\n",
    "                    url_list.append(img_url.group(1))\n",
    "\n",
    "    for url in url_list:\n",
    "        try:\n",
    "            img_path = make_img_path(save_dir_path, url)\n",
    "            image = download_image(url)\n",
    "            save_image(img_path, image)\n",
    "            print('saved image... {}'.format(url))\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except Exception as err:\n",
    "            print(\"%s\" % (err))\n",
    "\n",
    "    correspondence_table_path = os.path.join(save_dir_path, 'corr_table')\n",
    "    make_dir(correspondence_table_path)\n",
    "\n",
    "    with open(os.path.join(correspondence_table_path, 'corr_table.json'), mode='w') as f:\n",
    "        json.dump(correspondence_table, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
